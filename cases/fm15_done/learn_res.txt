--------------------------first try--------------------------------------------------------
restart: 0 epoch: 11 batch: 4095 batch_loss: 0.0 batch_gradient: 0.8810284909736644 epoch_loss: 0.0 

The last epoch: 11
Success! The nn model is:

tensor([[-0.4653,  0.1081],
        [-0.3737, -0.7920],
        [ 0.3640,  0.2028],
        [-0.0660,  2.2128],
        [-0.3080, -1.4103],
        [ 1.4396, -0.9942],
        [ 0.2135,  0.1343],
        [-0.0445, -0.6347],
        [ 0.8569,  0.1995],
        [ 0.0503, -0.3009]])
tensor([-1.1673,  0.4305,  0.1605, -0.9308, -0.6388, -0.3913,  0.1187, -0.4241,
        -0.6582,  0.3746])
tensor([[ 0.2336,  0.3948,  0.0377,  0.3982, -0.2806,  0.4383,  0.0272, -0.2960,
          0.3848, -0.2622]])
tensor([-0.4504])
restart: 0

Data generation totally cost: 0.2756052017211914
Training totally cost: 181.85268878936768

--------------parameters-----------:
import torch
import numpy as np
from functools import reduce
from operator import mul

############################################
# set default data type to double; for GPU
# training use float
############################################
torch.set_default_dtype(torch.float64)
torch.set_default_tensor_type(torch.DoubleTensor)
# torch.set_default_dtype(torch.float32)
# torch.set_default_tensor_type(torc.h.FloatTensor)


# for output
VERBOSE = 1


############################################
# set the network architecture
############################################
D_H = 10 # the number of neurons of each hidden layer
N_H = 1 # then number of hidden layers


############################################
# for activation function definition
############################################
BENT_DEG = 0.0001


############################################
# set loss function definition
############################################
TOL_INIT = 0.0
TOL_SAFE = 0.0
TOL_BOUNDARY = 0.05
TOL_LIE = 0.001
TOL_NORM_LIE = 0.0
WEIGHT_LIE = 1
WEIGHT_NORM_LIE = 0

DECAY_LIE = 1
DECAY_INIT = 1
DECAY_UNSAFE = 1


############################################
# for optimization method tunning: LBFGS
############################################
LBFGS_NUM_ITER = 1
LBFGS_TOL_GRAD = 1e-05
LBFGS_TOL_CHANGE = 1e-09
LBFGS_NUM_HISTORY = 100
LBFGS_LINE_SEARCH_FUN = None

# fine tuning using LBFGS
TOL_OPTIMIZER_RESET = -1
SHRINK_RATE_FACTOR = 10
FRACTION_INSTABLE_BATCH = 1000000000000000000000
NUM_BATCH_ITR = 3


############################################
# number of training epochs
############################################
EPOCHS = 500


############################################
# my own scheduling policy: 
# rate = alpha / (1 + beta * epoch^gamma)
############################################
ALPHA = 0.1 # initial learning rate
BETA = 0 # if beta equals 0 then constant rate = alpha
GAMMA = 0 # when beta is nonzero, larger gamma gives faster drop of rate


############################################
# training termination flags
############################################
LOSS_OPT_FLAG = 1e-16
TOL_MAX_GRAD = 6


############################################
# for training set generation
############################################
TOL_DATA_GEN = 1e-16

DATA_EXP_I = np.array([6, 6]) # for sampling from initial; length = prob.DIM
DATA_LEN_I = np.power(2, DATA_EXP_I) # the number of samples for each dimension of domain
BLOCK_EXP_I = np.array([4, 4]) # 0 <= BATCH_EXP <= DATA_EXP
BLOCK_LEN_I = np.power(2, BLOCK_EXP_I) # number of batches for each dimension

DATA_EXP_U = np.array([6, 6]) # for sampling from initial; length = prob.DIM
DATA_LEN_U = np.power(2, DATA_EXP_U) # the number of samples for each dimension of domain
BLOCK_EXP_U = np.array([4, 4]) # 0 <= BATCH_EXP <= DATA_EXP
BLOCK_LEN_U = np.power(2, BLOCK_EXP_U) # number of batches for each dimension

DATA_EXP_D = np.array([8, 8]) # for sampling from initial; length = prob.DIM
DATA_LEN_D = np.power(2, DATA_EXP_D) # the number of samples for each dimension of domain
BLOCK_EXP_D = np.array([6, 6]) # 0 <= BATCH_EXP <= DATA_EXP
BLOCK_LEN_D = np.power(2, BLOCK_EXP_D) # number of batches for each dimension


############################################
# number of mini_batches
############################################
BATCHES_I = reduce(mul, list(BLOCK_LEN_I))
BATCHES_U = reduce(mul, list(BLOCK_LEN_U))
BATCHES_D = reduce(mul, list(BLOCK_LEN_D))

BATCHES = max(BATCHES_I, BATCHES_U, BATCHES_D)


############################################
# for plotting
############################################
PLOT_EXP_B = np.array([8, 8]) # sampling from domain for plotting the boundary of barrier using contour plot
PLOT_LEN_B = np.power(2, PLOT_EXP_B) # the number of samples for each dimension of domain, usually larger than superp.DATA_LEN_D

PLOT_EXP_V = np.array([7, 7]) # sampling from domain for plotting the vector field
PLOT_LEN_V = np.power(2, PLOT_EXP_V) # the number of samples for each dimension of domain, usually equal to PLOT_LEN_P

PLOT_EXP_P = np.array([7, 7]) # sampling from domain for plotting the scattering sampling points, should be equal to superp.DATA_LEN_D
PLOT_LEN_P = np.power(2, PLOT_EXP_P) # the number of samples for each dimension of domain

PLOT_VEC_SCALE = None

---------------------------fine tune 1 from first try---------------------------------

restart: 0 epoch: 1 batch: 4095 batch_loss: 0.0 batch_gradient: 0.6702153092596116 epoch_loss: 0.0 

The last epoch: 1
Success! The nn model is:

tensor([[-0.4653,  0.1081],
        [-0.3737, -0.7920],
        [ 0.3637,  0.2028],
        [-0.0690,  2.2128],
        [-0.3080, -1.4103],
        [ 1.4396, -0.9942],
        [ 0.2133,  0.1343],
        [-0.0445, -0.6347],
        [ 0.8569,  0.1995],
        [ 0.0522, -0.3009]])
tensor([-1.1673,  0.4305,  0.1605, -0.9308, -0.6388, -0.3913,  0.1187, -0.4241,
        -0.6582,  0.3746])
tensor([[ 0.2336,  0.3948,  0.0350,  0.3987, -0.2806,  0.4383,  0.0256, -0.2960,
          0.3848, -0.2626]])
tensor([-0.4504])
restart: 0

Data generation totally cost: 0.16755342483520508
Training totally cost: 14.971669673919678

-------------------parameters:---------------------------
import torch
import numpy as np
from functools import reduce
from operator import mul

############################################
# set default data type to double; for GPU
# training use float
############################################
torch.set_default_dtype(torch.float64)
torch.set_default_tensor_type(torch.DoubleTensor)
# torch.set_default_dtype(torch.float32)
# torch.set_default_tensor_type(torc.h.FloatTensor)


# for output
VERBOSE = 1


############################################
# set the network architecture
############################################
D_H = 10 # the number of neurons of each hidden layer
N_H = 1 # then number of hidden layers


############################################
# for activation function definition
############################################
BENT_DEG = 0.0001


############################################
# set loss function definition
############################################
TOL_INIT = 0.01
TOL_SAFE = 0.01
TOL_BOUNDARY = 0.05
TOL_LIE = 0.02
TOL_NORM_LIE = 0.0
WEIGHT_LIE = 1
WEIGHT_NORM_LIE = 0

DECAY_LIE = 1
DECAY_INIT = 1
DECAY_UNSAFE = 1


############################################
# for optimization method tunning: LBFGS
############################################
LBFGS_NUM_ITER = 1
LBFGS_TOL_GRAD = 1e-05
LBFGS_TOL_CHANGE = 1e-09
LBFGS_NUM_HISTORY = 100
LBFGS_LINE_SEARCH_FUN = None

# fine tuning using LBFGS
TOL_OPTIMIZER_RESET = -1
SHRINK_RATE_FACTOR = 10
FRACTION_INSTABLE_BATCH = 1000000000000000000000
NUM_BATCH_ITR = 3


############################################
# number of training epochs
############################################
EPOCHS = 500


############################################
# my own scheduling policy: 
# rate = alpha / (1 + beta * epoch^gamma)
############################################
ALPHA = 0.01 # initial learning rate
BETA = 0 # if beta equals 0 then constant rate = alpha
GAMMA = 0 # when beta is nonzero, larger gamma gives faster drop of rate


############################################
# training termination flags
############################################
LOSS_OPT_FLAG = 1e-16
TOL_MAX_GRAD = 6


############################################
# for training set generation
############################################
TOL_DATA_GEN = 1e-16

DATA_EXP_I = np.array([6, 6]) # for sampling from initial; length = prob.DIM
DATA_LEN_I = np.power(2, DATA_EXP_I) # the number of samples for each dimension of domain
BLOCK_EXP_I = np.array([4, 4]) # 0 <= BATCH_EXP <= DATA_EXP
BLOCK_LEN_I = np.power(2, BLOCK_EXP_I) # number of batches for each dimension

DATA_EXP_U = np.array([6, 6]) # for sampling from initial; length = prob.DIM
DATA_LEN_U = np.power(2, DATA_EXP_U) # the number of samples for each dimension of domain
BLOCK_EXP_U = np.array([4, 4]) # 0 <= BATCH_EXP <= DATA_EXP
BLOCK_LEN_U = np.power(2, BLOCK_EXP_U) # number of batches for each dimension

DATA_EXP_D = np.array([8, 8]) # for sampling from initial; length = prob.DIM
DATA_LEN_D = np.power(2, DATA_EXP_D) # the number of samples for each dimension of domain
BLOCK_EXP_D = np.array([6, 6]) # 0 <= BATCH_EXP <= DATA_EXP
BLOCK_LEN_D = np.power(2, BLOCK_EXP_D) # number of batches for each dimension


############################################
# number of mini_batches
############################################
BATCHES_I = reduce(mul, list(BLOCK_LEN_I))
BATCHES_U = reduce(mul, list(BLOCK_LEN_U))
BATCHES_D = reduce(mul, list(BLOCK_LEN_D))

BATCHES = max(BATCHES_I, BATCHES_U, BATCHES_D)


############################################
# for plotting
############################################
PLOT_EXP_B = np.array([8, 8]) # sampling from domain for plotting the boundary of barrier using contour plot
PLOT_LEN_B = np.power(2, PLOT_EXP_B) # the number of samples for each dimension of domain, usually larger than superp.DATA_LEN_D

PLOT_EXP_V = np.array([7, 7]) # sampling from domain for plotting the vector field
PLOT_LEN_V = np.power(2, PLOT_EXP_V) # the number of samples for each dimension of domain, usually equal to PLOT_LEN_P

PLOT_EXP_P = np.array([7, 7]) # sampling from domain for plotting the scattering sampling points, should be equal to superp.DATA_LEN_D
PLOT_LEN_P = np.power(2, PLOT_EXP_P) # the number of samples for each dimension of domain

PLOT_VEC_SCALE = None

-------------------------------fine tune 2 from 1--------------------------------
restart: 0 epoch: 7 batch: 4095 batch_loss: 0.0 batch_gradient: 0.5669856536891583 epoch_loss: 0.0 

The last epoch: 7
Success! The nn model is:

tensor([[-0.4653,  0.1081],
        [-0.3665, -0.7950],
        [ 0.3614,  0.2032],
        [-0.0959,  2.2195],
        [-0.3080, -1.4103],
        [ 1.4549, -0.9734],
        [ 0.2114,  0.1347],
        [-0.0445, -0.6347],
        [ 0.8692,  0.2199],
        [ 0.0809, -0.3093]])
tensor([-1.1673,  0.4399,  0.1606, -0.9101, -0.6388, -0.3809,  0.1188, -0.4241,
        -0.6512,  0.3733])
tensor([[ 0.2336,  0.4043,  0.0036,  0.3972, -0.2806,  0.4329,  0.0079, -0.2960,
          0.4111, -0.2770]])
tensor([-0.4434])
restart: 0

Data generation totally cost: 0.15942811965942383
Training totally cost: 63.726810932159424


parameters:---------------------------
import torch
import numpy as np
from functools import reduce
from operator import mul

############################################
# set default data type to double; for GPU
# training use float
############################################
torch.set_default_dtype(torch.float64)
torch.set_default_tensor_type(torch.DoubleTensor)
# torch.set_default_dtype(torch.float32)
# torch.set_default_tensor_type(torc.h.FloatTensor)


# for output
VERBOSE = 1


############################################
# set the network architecture
############################################
D_H = 10 # the number of neurons of each hidden layer
N_H = 1 # then number of hidden layers


############################################
# for activation function definition
############################################
BENT_DEG = 0.0001


############################################
# set loss function definition
############################################
TOL_INIT = 0.01
TOL_SAFE = 0.01
TOL_BOUNDARY = 0.05
TOL_LIE = 0.05
TOL_NORM_LIE = 0.0
WEIGHT_LIE = 1
WEIGHT_NORM_LIE = 0

DECAY_LIE = 1
DECAY_INIT = 1
DECAY_UNSAFE = 1


############################################
# for optimization method tunning: LBFGS
############################################
LBFGS_NUM_ITER = 1
LBFGS_TOL_GRAD = 1e-05
LBFGS_TOL_CHANGE = 1e-09
LBFGS_NUM_HISTORY = 100
LBFGS_LINE_SEARCH_FUN = None

# fine tuning using LBFGS
TOL_OPTIMIZER_RESET = -1
SHRINK_RATE_FACTOR = 10
FRACTION_INSTABLE_BATCH = 1000000000000000000000
NUM_BATCH_ITR = 3


############################################
# number of training epochs
############################################
EPOCHS = 500


############################################
# my own scheduling policy: 
# rate = alpha / (1 + beta * epoch^gamma)
############################################
ALPHA = 0.01 # initial learning rate
BETA = 0 # if beta equals 0 then constant rate = alpha
GAMMA = 0 # when beta is nonzero, larger gamma gives faster drop of rate


############################################
# training termination flags
############################################
LOSS_OPT_FLAG = 1e-16
TOL_MAX_GRAD = 6


############################################
# for training set generation
############################################
TOL_DATA_GEN = 1e-16

DATA_EXP_I = np.array([6, 6]) # for sampling from initial; length = prob.DIM
DATA_LEN_I = np.power(2, DATA_EXP_I) # the number of samples for each dimension of domain
BLOCK_EXP_I = np.array([4, 4]) # 0 <= BATCH_EXP <= DATA_EXP
BLOCK_LEN_I = np.power(2, BLOCK_EXP_I) # number of batches for each dimension

DATA_EXP_U = np.array([6, 6]) # for sampling from initial; length = prob.DIM
DATA_LEN_U = np.power(2, DATA_EXP_U) # the number of samples for each dimension of domain
BLOCK_EXP_U = np.array([4, 4]) # 0 <= BATCH_EXP <= DATA_EXP
BLOCK_LEN_U = np.power(2, BLOCK_EXP_U) # number of batches for each dimension

DATA_EXP_D = np.array([8, 8]) # for sampling from initial; length = prob.DIM
DATA_LEN_D = np.power(2, DATA_EXP_D) # the number of samples for each dimension of domain
BLOCK_EXP_D = np.array([6, 6]) # 0 <= BATCH_EXP <= DATA_EXP
BLOCK_LEN_D = np.power(2, BLOCK_EXP_D) # number of batches for each dimension


############################################
# number of mini_batches
############################################
BATCHES_I = reduce(mul, list(BLOCK_LEN_I))
BATCHES_U = reduce(mul, list(BLOCK_LEN_U))
BATCHES_D = reduce(mul, list(BLOCK_LEN_D))

BATCHES = max(BATCHES_I, BATCHES_U, BATCHES_D)


############################################
# for plotting
############################################
PLOT_EXP_B = np.array([8, 8]) # sampling from domain for plotting the boundary of barrier using contour plot
PLOT_LEN_B = np.power(2, PLOT_EXP_B) # the number of samples for each dimension of domain, usually larger than superp.DATA_LEN_D

PLOT_EXP_V = np.array([7, 7]) # sampling from domain for plotting the vector field
PLOT_LEN_V = np.power(2, PLOT_EXP_V) # the number of samples for each dimension of domain, usually equal to PLOT_LEN_P

PLOT_EXP_P = np.array([7, 7]) # sampling from domain for plotting the scattering sampling points, should be equal to superp.DATA_LEN_D
PLOT_LEN_P = np.power(2, PLOT_EXP_P) # the number of samples for each dimension of domain

PLOT_VEC_SCALE = None

---------------------------fine tune 3 from 2------------------------
restart: 0 epoch: 3 batch: 4095 batch_loss: 0.0 batch_gradient: 0.5750083786779506 epoch_loss: 0.0 

The last epoch: 3
Success! The nn model is:

tensor([[-0.4653,  0.1081],
        [-0.3637, -0.7959],
        [ 0.3614,  0.2032],
        [-0.0946,  2.2188],
        [-0.3080, -1.4103],
        [ 1.4587, -0.9715],
        [ 0.2114,  0.1347],
        [-0.0445, -0.6347],
        [ 0.8729,  0.2218],
        [ 0.0812, -0.3085]])
tensor([-1.1673,  0.4431,  0.1606, -0.9086, -0.6388, -0.3774,  0.1188, -0.4241,
        -0.6474,  0.3733])
tensor([[ 0.2336,  0.4069,  0.0027,  0.3900, -0.2806,  0.4384,  0.0073, -0.2960,
          0.4143, -0.2762]])
tensor([-0.4434])
restart: 0

Data generation totally cost: 0.1525423526763916
Training totally cost: 29.845974445343018


parameters:-----------------------
import torch
import numpy as np
from functools import reduce
from operator import mul

############################################
# set default data type to double; for GPU
# training use float
############################################
torch.set_default_dtype(torch.float64)
torch.set_default_tensor_type(torch.DoubleTensor)
# torch.set_default_dtype(torch.float32)
# torch.set_default_tensor_type(torc.h.FloatTensor)


# for output
VERBOSE = 1


############################################
# set the network architecture
############################################
D_H = 10 # the number of neurons of each hidden layer
N_H = 1 # then number of hidden layers


############################################
# for activation function definition
############################################
BENT_DEG = 0.0001


############################################
# set loss function definition
############################################
TOL_INIT = 0.02
TOL_SAFE = 0.02
TOL_BOUNDARY = 0.05
TOL_LIE = 0.05
TOL_NORM_LIE = 0.0
WEIGHT_LIE = 1
WEIGHT_NORM_LIE = 0

DECAY_LIE = 1
DECAY_INIT = 1
DECAY_UNSAFE = 1


############################################
# for optimization method tunning: LBFGS
############################################
LBFGS_NUM_ITER = 1
LBFGS_TOL_GRAD = 1e-05
LBFGS_TOL_CHANGE = 1e-09
LBFGS_NUM_HISTORY = 100
LBFGS_LINE_SEARCH_FUN = None

# fine tuning using LBFGS
TOL_OPTIMIZER_RESET = -1
SHRINK_RATE_FACTOR = 10
FRACTION_INSTABLE_BATCH = 1000000000000000000000
NUM_BATCH_ITR = 3


############################################
# number of training epochs
############################################
EPOCHS = 500


############################################
# my own scheduling policy: 
# rate = alpha / (1 + beta * epoch^gamma)
############################################
ALPHA = 0.01 # initial learning rate
BETA = 0 # if beta equals 0 then constant rate = alpha
GAMMA = 0 # when beta is nonzero, larger gamma gives faster drop of rate


############################################
# training termination flags
############################################
LOSS_OPT_FLAG = 1e-16
TOL_MAX_GRAD = 6


############################################
# for training set generation
############################################
TOL_DATA_GEN = 1e-16

DATA_EXP_I = np.array([6, 6]) # for sampling from initial; length = prob.DIM
DATA_LEN_I = np.power(2, DATA_EXP_I) # the number of samples for each dimension of domain
BLOCK_EXP_I = np.array([4, 4]) # 0 <= BATCH_EXP <= DATA_EXP
BLOCK_LEN_I = np.power(2, BLOCK_EXP_I) # number of batches for each dimension

DATA_EXP_U = np.array([6, 6]) # for sampling from initial; length = prob.DIM
DATA_LEN_U = np.power(2, DATA_EXP_U) # the number of samples for each dimension of domain
BLOCK_EXP_U = np.array([4, 4]) # 0 <= BATCH_EXP <= DATA_EXP
BLOCK_LEN_U = np.power(2, BLOCK_EXP_U) # number of batches for each dimension

DATA_EXP_D = np.array([8, 8]) # for sampling from initial; length = prob.DIM
DATA_LEN_D = np.power(2, DATA_EXP_D) # the number of samples for each dimension of domain
BLOCK_EXP_D = np.array([6, 6]) # 0 <= BATCH_EXP <= DATA_EXP
BLOCK_LEN_D = np.power(2, BLOCK_EXP_D) # number of batches for each dimension


############################################
# number of mini_batches
############################################
BATCHES_I = reduce(mul, list(BLOCK_LEN_I))
BATCHES_U = reduce(mul, list(BLOCK_LEN_U))
BATCHES_D = reduce(mul, list(BLOCK_LEN_D))

BATCHES = max(BATCHES_I, BATCHES_U, BATCHES_D)


############################################
# for plotting
############################################
PLOT_EXP_B = np.array([8, 8]) # sampling from domain for plotting the boundary of barrier using contour plot
PLOT_LEN_B = np.power(2, PLOT_EXP_B) # the number of samples for each dimension of domain, usually larger than superp.DATA_LEN_D

PLOT_EXP_V = np.array([7, 7]) # sampling from domain for plotting the vector field
PLOT_LEN_V = np.power(2, PLOT_EXP_V) # the number of samples for each dimension of domain, usually equal to PLOT_LEN_P

PLOT_EXP_P = np.array([7, 7]) # sampling from domain for plotting the scattering sampling points, should be equal to superp.DATA_LEN_D
PLOT_LEN_P = np.power(2, PLOT_EXP_P) # the number of samples for each dimension of domain

PLOT_VEC_SCALE = None

-------------------fine tune 4 from 3----------------------------------
restart: 0 epoch: 44 batch: 4095 batch_loss: 0.0 batch_gradient: 0.9219648879264044 epoch_loss: 0.0 

The last epoch: 44
Success! The nn model is:

tensor([[-0.4653,  0.1081],
        [-0.3138, -0.8143],
        [ 0.3602,  0.2026],
        [-0.1345,  2.2256],
        [-0.3082, -1.4101],
        [ 1.4778, -0.9532],
        [ 0.2101,  0.1352],
        [-0.0446, -0.6345],
        [ 0.8990,  0.2262],
        [ 0.0957, -0.3290]])
tensor([-1.1673,  0.4748,  0.1624, -0.8979, -0.6393, -0.4055,  0.1200, -0.4244,
        -0.6775,  0.3927])
tensor([[ 0.2336,  0.4426, -0.0067,  0.4141, -0.2804,  0.4992,  0.0024, -0.2959,
          0.5253, -0.3306]])
tensor([-0.4847])
restart: 0

Data generation totally cost: 0.18304061889648438
Training totally cost: 628.115564584732


import torch
import numpy as np
from functools import reduce
from operator import mul

############################################
# set default data type to double; for GPU
# training use float
############################################
torch.set_default_dtype(torch.float64)
torch.set_default_tensor_type(torch.DoubleTensor)
# torch.set_default_dtype(torch.float32)
# torch.set_default_tensor_type(torc.h.FloatTensor)


# for output
VERBOSE = 1


############################################
# set the network architecture
############################################
D_H = 10 # the number of neurons of each hidden layer
N_H = 1 # then number of hidden layers


############################################
# for activation function definition
############################################
BENT_DEG = 0.0001


############################################
# set loss function definition
############################################
TOL_INIT = 0.02
TOL_SAFE = 0.02
TOL_BOUNDARY = 0.05
TOL_LIE = 0.08
TOL_NORM_LIE = 0.0
WEIGHT_LIE = 1
WEIGHT_NORM_LIE = 0

DECAY_LIE = 1
DECAY_INIT = 1
DECAY_UNSAFE = 1


############################################
# for optimization method tunning: LBFGS
############################################
LBFGS_NUM_ITER = 1
LBFGS_TOL_GRAD = 1e-05
LBFGS_TOL_CHANGE = 1e-09
LBFGS_NUM_HISTORY = 100
LBFGS_LINE_SEARCH_FUN = None

# fine tuning using LBFGS
TOL_OPTIMIZER_RESET = 0.1
SHRINK_RATE_FACTOR = 10
FRACTION_INSTABLE_BATCH = 1000000000000000000000
NUM_BATCH_ITR = 3


############################################
# number of training epochs
############################################
EPOCHS = 500


############################################
# my own scheduling policy: 
# rate = alpha / (1 + beta * epoch^gamma)
############################################
ALPHA = 0.01 # initial learning rate
BETA = 0 # if beta equals 0 then constant rate = alpha
GAMMA = 0 # when beta is nonzero, larger gamma gives faster drop of rate


############################################
# training termination flags
############################################
LOSS_OPT_FLAG = 1e-16
TOL_MAX_GRAD = 6


############################################
# for training set generation
############################################
TOL_DATA_GEN = 1e-16

DATA_EXP_I = np.array([6, 6]) # for sampling from initial; length = prob.DIM
DATA_LEN_I = np.power(2, DATA_EXP_I) # the number of samples for each dimension of domain
BLOCK_EXP_I = np.array([4, 4]) # 0 <= BATCH_EXP <= DATA_EXP
BLOCK_LEN_I = np.power(2, BLOCK_EXP_I) # number of batches for each dimension

DATA_EXP_U = np.array([6, 6]) # for sampling from initial; length = prob.DIM
DATA_LEN_U = np.power(2, DATA_EXP_U) # the number of samples for each dimension of domain
BLOCK_EXP_U = np.array([4, 4]) # 0 <= BATCH_EXP <= DATA_EXP
BLOCK_LEN_U = np.power(2, BLOCK_EXP_U) # number of batches for each dimension

DATA_EXP_D = np.array([8, 8]) # for sampling from initial; length = prob.DIM
DATA_LEN_D = np.power(2, DATA_EXP_D) # the number of samples for each dimension of domain
BLOCK_EXP_D = np.array([6, 6]) # 0 <= BATCH_EXP <= DATA_EXP
BLOCK_LEN_D = np.power(2, BLOCK_EXP_D) # number of batches for each dimension


############################################
# number of mini_batches
############################################
BATCHES_I = reduce(mul, list(BLOCK_LEN_I))
BATCHES_U = reduce(mul, list(BLOCK_LEN_U))
BATCHES_D = reduce(mul, list(BLOCK_LEN_D))

BATCHES = max(BATCHES_I, BATCHES_U, BATCHES_D)


############################################
# for plotting
############################################
PLOT_EXP_B = np.array([8, 8]) # sampling from domain for plotting the boundary of barrier using contour plot
PLOT_LEN_B = np.power(2, PLOT_EXP_B) # the number of samples for each dimension of domain, usually larger than superp.DATA_LEN_D

PLOT_EXP_V = np.array([7, 7]) # sampling from domain for plotting the vector field
PLOT_LEN_V = np.power(2, PLOT_EXP_V) # the number of samples for each dimension of domain, usually equal to PLOT_LEN_P

PLOT_EXP_P = np.array([7, 7]) # sampling from domain for plotting the scattering sampling points, should be equal to superp.DATA_LEN_D
PLOT_LEN_P = np.power(2, PLOT_EXP_P) # the number of samples for each dimension of domain

PLOT_VEC_SCALE = None


----------------------fine tune 5 from 4-----------------------------

restart: 0 epoch: 58 batch: 4095 batch_loss: 0.0 batch_gradient: 1.0156399932098554 epoch_loss: 0.0 

The last epoch: 58
Success! The nn model is:

tensor([[-0.4653,  0.1081],
        [-0.2802, -0.8425],
        [ 0.3592,  0.2013],
        [-0.1568,  2.2246],
        [-0.3086, -1.4097],
        [ 1.4852, -0.9569],
        [ 0.2094,  0.1350],
        [-0.0448, -0.6343],
        [ 0.9277,  0.2325],
        [ 0.1044, -0.3599]])
tensor([-1.1673,  0.4850,  0.1627, -0.9033, -0.6401, -0.4655,  0.1198, -0.4248,
        -0.7313,  0.4120])
tensor([[ 0.2336,  0.4914, -0.0023,  0.4555, -0.2802,  0.5934,  0.0051, -0.2958,
          0.6572, -0.3827]])
tensor([-0.5387])
restart: 0

Data generation totally cost: 0.15534710884094238
Training totally cost: 517.6738269329071



import torch
import numpy as np
from functools import reduce
from operator import mul

############################################
# set default data type to double; for GPU
# training use float
############################################
torch.set_default_dtype(torch.float64)
torch.set_default_tensor_type(torch.DoubleTensor)
# torch.set_default_dtype(torch.float32)
# torch.set_default_tensor_type(torc.h.FloatTensor)


# for output
VERBOSE = 1


############################################
# set the network architecture
############################################
D_H = 10 # the number of neurons of each hidden layer
N_H = 1 # then number of hidden layers


############################################
# for activation function definition
############################################
BENT_DEG = 0.0001


############################################
# set loss function definition
############################################
TOL_INIT = 0.02
TOL_SAFE = 0.02
TOL_BOUNDARY = 0.05
TOL_LIE = 0.10
TOL_NORM_LIE = 0.0
WEIGHT_LIE = 1
WEIGHT_NORM_LIE = 0

DECAY_LIE = 1
DECAY_INIT = 1
DECAY_UNSAFE = 1


############################################
# for optimization method tunning: LBFGS
############################################
LBFGS_NUM_ITER = 1
LBFGS_TOL_GRAD = 1e-05
LBFGS_TOL_CHANGE = 1e-09
LBFGS_NUM_HISTORY = 100
LBFGS_LINE_SEARCH_FUN = None

# fine tuning using LBFGS
TOL_OPTIMIZER_RESET = -1
SHRINK_RATE_FACTOR = 10
FRACTION_INSTABLE_BATCH = 1000000000000000000000
NUM_BATCH_ITR = 3


############################################
# number of training epochs
############################################
EPOCHS = 500


############################################
# my own scheduling policy: 
# rate = alpha / (1 + beta * epoch^gamma)
############################################
ALPHA = 0.01 # initial learning rate
BETA = 0 # if beta equals 0 then constant rate = alpha
GAMMA = 0 # when beta is nonzero, larger gamma gives faster drop of rate


############################################
# training termination flags
############################################
LOSS_OPT_FLAG = 1e-16
TOL_MAX_GRAD = 6


############################################
# for training set generation
############################################
TOL_DATA_GEN = 1e-16

DATA_EXP_I = np.array([6, 6]) # for sampling from initial; length = prob.DIM
DATA_LEN_I = np.power(2, DATA_EXP_I) # the number of samples for each dimension of domain
BLOCK_EXP_I = np.array([4, 4]) # 0 <= BATCH_EXP <= DATA_EXP
BLOCK_LEN_I = np.power(2, BLOCK_EXP_I) # number of batches for each dimension

DATA_EXP_U = np.array([6, 6]) # for sampling from initial; length = prob.DIM
DATA_LEN_U = np.power(2, DATA_EXP_U) # the number of samples for each dimension of domain
BLOCK_EXP_U = np.array([4, 4]) # 0 <= BATCH_EXP <= DATA_EXP
BLOCK_LEN_U = np.power(2, BLOCK_EXP_U) # number of batches for each dimension

DATA_EXP_D = np.array([8, 8]) # for sampling from initial; length = prob.DIM
DATA_LEN_D = np.power(2, DATA_EXP_D) # the number of samples for each dimension of domain
BLOCK_EXP_D = np.array([6, 6]) # 0 <= BATCH_EXP <= DATA_EXP
BLOCK_LEN_D = np.power(2, BLOCK_EXP_D) # number of batches for each dimension


############################################
# number of mini_batches
############################################
BATCHES_I = reduce(mul, list(BLOCK_LEN_I))
BATCHES_U = reduce(mul, list(BLOCK_LEN_U))
BATCHES_D = reduce(mul, list(BLOCK_LEN_D))

BATCHES = max(BATCHES_I, BATCHES_U, BATCHES_D)


############################################
# for plotting
############################################
PLOT_EXP_B = np.array([8, 8]) # sampling from domain for plotting the boundary of barrier using contour plot
PLOT_LEN_B = np.power(2, PLOT_EXP_B) # the number of samples for each dimension of domain, usually larger than superp.DATA_LEN_D

PLOT_EXP_V = np.array([7, 7]) # sampling from domain for plotting the vector field
PLOT_LEN_V = np.power(2, PLOT_EXP_V) # the number of samples for each dimension of domain, usually equal to PLOT_LEN_P

PLOT_EXP_P = np.array([7, 7]) # sampling from domain for plotting the scattering sampling points, should be equal to superp.DATA_LEN_D
PLOT_LEN_P = np.power(2, PLOT_EXP_P) # the number of samples for each dimension of domain

PLOT_VEC_SCALE = None